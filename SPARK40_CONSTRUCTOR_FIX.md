# Spark 4.0 Constructor Compatibility Fix

## Problem Summary

After deploying the Spark 4.0-compiled connector, the following runtime error occurred:

```
java.lang.NoSuchMethodError: 'void org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(org.apache.spark.sql.catalyst.util.CaseInsensitiveMap)'
    at com.microsoft.sqlserver.jdbc.spark.SQLServerBulkJdbcOptions.<init>(SQLServerBulkJdbcOptions.scala:25)
    at com.microsoft.sqlserver.jdbc.spark.SQLServerBulkJdbcOptions.<init>(SQLServerBulkJdbcOptions.scala:27)
    at com.microsoft.sqlserver.jdbc.spark.DefaultSource.createRelation(DefaultSource.scala:55)
```

## Root Cause Analysis

The issue was in `/home/azureuser/sql-spark-connector/src/main/scala/com/microsoft/sqlserver/jdbc/spark/SQLServerBulkJdbcOptions.scala`.

### Original Code (Lines 24-32)

```scala
class SQLServerBulkJdbcOptions(val params: CaseInsensitiveMap[String])
    extends JdbcOptionsInWrite(params) {

  def this(params: Map[String, String]) = this(CaseInsensitiveMap(params))

  // Save original parameters for when a JdbcBulkOptions instance is passed
  // from the Spark driver to an executor, which loses the reference to the
  // params input in memory
  override val parameters = params
```

### The Problem

The original code had two issues that caused binary incompatibility with Spark 4.0:

1. **Double field declaration**: The class constructor declared `val params: CaseInsensitiveMap[String]` AND tried to override `parameters` with the same value
2. **Conflicting override**: `JdbcOptionsInWrite` already has `override val parameters: CaseInsensitiveMap[String]` in its constructor signature in Spark 4.0
3. **Initialization order**: The Scala compiler generated bytecode that attempted to set `parameters` twice - once through inheritance and once through the override

This pattern worked in Spark 3.4 but created a binary incompatibility in Spark 4.0 due to changes in how Scala 2.13 handles constructor parameter inheritance.

## The Fix

### Updated Code (Lines 24-31)

```scala
class SQLServerBulkJdbcOptions(
    parameters: CaseInsensitiveMap[String])
    extends JdbcOptionsInWrite(parameters) {

  def this(params: Map[String, String]) = this(CaseInsensitiveMap(params))

  // Keep reference to parameters for later use
  val params: CaseInsensitiveMap[String] = parameters
```

### Key Changes

1. **Removed `val` from constructor parameter**: Changed from `val params:` to just `parameters:` - this makes it a regular constructor parameter, not a field
2. **Removed `override val parameters`**: Eliminated the conflicting override statement
3. **Created local field `params`**: Added `val params = parameters` to maintain the existing API (rest of the code uses `params` not `parameters`)

### Why This Works

1. The constructor now cleanly passes `parameters` to the parent `JdbcOptionsInWrite` without trying to override anything
2. The parent class handles the `parameters` field through its own `override val parameters` in its constructor
3. The local `params` field provides a reference for the rest of the class code
4. No binary incompatibility because we're not fighting with the parent class's field initialization

## Verification

### Build Command

```bash
cd /home/azureuser/sql-spark-connector
mvn clean package -Pspark40 -Dmaven.javadoc.skip=true -Djar.finalName=spark-mssql-connector_2.13-1.4.0-spark40
```

### Build Results

- **Status**: BUILD SUCCESS
- **Output**: `/home/azureuser/sql-spark-connector/target/spark-mssql-connector_2.13-1.4.0-spark40.jar`
- **Size**: 84K

### Bytecode Verification

The corrected constructor bytecode shows the proper call sequence:

```
public com.microsoft.sqlserver.jdbc.spark.SQLServerBulkJdbcOptions(
    org.apache.spark.sql.catalyst.util.CaseInsensitiveMap<java.lang.String>);
    Code:
       0: aload_0
       1: aload_1
       2: invokespecial #116  // Method JdbcOptionsInWrite."<init>":(CaseInsensitiveMap;)V
       5: aload_0
       6: aload_1
       7: putfield      #51   // Field params:CaseInsensitiveMap;
```

This shows:
- Line 2: Clean call to parent constructor with CaseInsensitiveMap
- Line 7: Store reference in local `params` field

## Testing Recommendations

Before deploying to production, verify:

1. **Spark version**: Ensure runtime environment uses Spark 4.0.x (not 3.x)
2. **Scala version**: Confirm Spark is built with Scala 2.13 (not 2.12)
3. **Classpath**: Verify no Spark 3.x JARs are on the classpath
4. **Basic write test**: Test a simple write operation to SQL Server
5. **Bulk copy test**: Verify bulk copy operations work correctly
6. **Data pool test**: If using data pools, test dataPoolDataSource parameter

## Related Files

- **Fixed file**: `/home/azureuser/sql-spark-connector/src/main/scala/com/microsoft/sqlserver/jdbc/spark/SQLServerBulkJdbcOptions.scala`
- **Caller**: `/home/azureuser/sql-spark-connector/src/main/scala/com/microsoft/sqlserver/jdbc/spark/DefaultSource.scala` (line 55)
- **Build output**: `/home/azureuser/sql-spark-connector/target/spark-mssql-connector_2.13-1.4.0-spark40.jar`

## Additional Context

This issue is specific to the Spark 3.4 → 4.0 migration and relates to:

1. **Scala 2.12 → 2.13 migration**: Stricter type checking and constructor initialization
2. **Spark JDBC API changes**: Internal changes to how `JdbcOptionsInWrite` handles parameters
3. **Binary compatibility**: Compiled code must match runtime method signatures exactly

The fix maintains backward compatibility with the existing connector API while ensuring forward compatibility with Spark 4.0.
